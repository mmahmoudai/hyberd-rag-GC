<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="updatedContent" value="# Network RAG Research&#10;&#10;**Contact:** Dr Muhammad M.Hanafy  &#10;**Email:** m.mahmoud@mau.edu.eg&#10;&#10;&lt;!-- ...existing content... --&gt;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/code_documentation.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/code_documentation.md" />
              <option name="originalContent" value="# Network Security RAG Comparison - Code Documentation&#10;&#10;This document provides comprehensive documentation for the code implementation of our network security RAG comparison research project. The documentation covers all components of the system, including models, data processing, evaluation, and visualization.&#10;&#10;## Table of Contents&#10;1. [Project Overview](#project-overview)&#10;2. [Installation](#installation)&#10;3. [Directory Structure](#directory-structure)&#10;4. [Models](#models)&#10;5. [Data Processing](#data-processing)&#10;6. [Evaluation](#evaluation)&#10;7. [Visualization](#visualization)&#10;8. [Experiments](#experiments)&#10;9. [API Reference](#api-reference)&#10;10. [Contributing](#contributing)&#10;&#10;## Project Overview&#10;&#10;This project implements and compares four Retrieval Augmented Generation (RAG) architectures for network security packet analysis:&#10;&#10;1. **Traditional RAG**: A vector-based retrieval system optimized for network security data&#10;2. **Graph RAG**: A graph-based approach that models network entities and their relationships&#10;3. **Cache RAG**: A performance-optimized system with strategic caching mechanisms&#10;4. **Hybrid Cache-Graph RAG**: A novel approach combining graph-based knowledge representation with caching benefits&#10;&#10;The project includes implementations of all four architectures, data processing pipelines, evaluation frameworks, and visualization tools.&#10;&#10;## Installation&#10;&#10;### Prerequisites&#10;- Python 3.8+&#10;- CUDA-compatible GPU (recommended for optimal performance)&#10;- 16GB+ RAM&#10;- 100GB+ disk space&#10;&#10;### Setup with Docker (Recommended)&#10;```bash&#10;# Clone the repository&#10;git clone https://github.com/network-rag-research/network-security-rag-comparison.git&#10;cd network-rag-research&#10;&#10;# Build and run the Docker container&#10;docker build -t network-rag-research .&#10;docker run -it --gpus all -v $(pwd):/network-rag-research network-rag-research&#10;```&#10;&#10;### Manual Setup&#10;```bash&#10;# Clone the repository&#10;git clone https://github.com/network-rag-research/network-security-rag-comparison.git&#10;cd network-rag-research&#10;&#10;# Create and activate a virtual environment&#10;python -m venv venv&#10;source venv/bin/activate  # On Windows: venv\Scripts\activate&#10;&#10;# Install dependencies&#10;pip install -r requirements.txt&#10;```&#10;&#10;## Directory Structure&#10;&#10;```&#10;/network-rag-research/&#10;├── src/&#10;│   ├── models/              # Implementation of all RAG variants&#10;│   ├── data_processing/     # Data preprocessing and loading utilities&#10;│   ├── evaluation/          # Benchmarking and testing code&#10;│   ├── visualization/       # Plotting and graph generation code&#10;│   └── utils/               # Helper functions and common utilities&#10;├── data/&#10;│   ├── raw/                 # Original datasets&#10;│   ├── processed/           # Cleaned and preprocessed data&#10;│   └── embeddings/          # Vector representations and indexes&#10;├── experiments/&#10;│   ├── configs/             # Configuration files for different experiments&#10;│   ├── logs/                # Training and evaluation logs&#10;│   └── results/             # Metrics and performance data&#10;└── docker/                  # Containerization files for reproducibility&#10;```&#10;&#10;## Models&#10;&#10;### Traditional RAG&#10;&#10;The Traditional RAG architecture follows the standard retrieval-augmented generation paradigm, adapted for network security packet analysis.&#10;&#10;#### Key Components&#10;&#10;- **Document Processing**: Converts network packets and flow records into document chunks&#10;- **Embedding Generation**: Encodes documents into dense vector representations&#10;- **Vector Index**: Stores embeddings for efficient similarity search&#10;- **Generation**: Uses retrieved documents as context for a language model&#10;&#10;#### Usage Example&#10;&#10;```python&#10;from src.models.traditional_rag import TraditionalRAG&#10;&#10;# Initialize the model&#10;trad_rag = TraditionalRAG(&#10;    embedding_dim=768,&#10;    chunk_strategy=&quot;sliding_window&quot;,&#10;    retrieval_method=&quot;dense&quot;&#10;)&#10;&#10;# Process documents&#10;trad_rag.process_documents(documents)&#10;&#10;# Query the system&#10;results = trad_rag.query(&quot;Detect potential DDoS attacks in this traffic&quot;)&#10;print(results)&#10;```&#10;&#10;### Graph RAG&#10;&#10;The Graph RAG architecture extends Traditional RAG by modeling network entities and their relationships as a knowledge graph.&#10;&#10;#### Key Components&#10;&#10;- **Knowledge Graph Construction**: Represents network entities as nodes in a heterogeneous graph&#10;- **Graph Embedding**: Generates node and edge embeddings using a graph neural network&#10;- **Graph Traversal Retrieval**: Performs multi-hop traversal to retrieve related entities&#10;- **Context Aggregation**: Aggregates information from retrieved nodes and their neighborhoods&#10;&#10;#### Usage Example&#10;&#10;```python&#10;from src.models.graph_rag import GraphRAG&#10;&#10;# Initialize the model&#10;graph_rag = GraphRAG(&#10;    graph_type=&quot;heterogeneous&quot;,&#10;    node_config=&quot;entity_attribute&quot;,&#10;    traversal_depth=2&#10;)&#10;&#10;# Build the knowledge graph&#10;graph_rag.build_graph(network_data)&#10;&#10;# Query the system&#10;results = graph_rag.query(&quot;Identify communication patterns between these IP addresses&quot;)&#10;print(results)&#10;```&#10;&#10;### Cache RAG&#10;&#10;The Cache RAG architecture focuses on performance optimization through strategic caching mechanisms.&#10;&#10;#### Key Components&#10;&#10;- **Query Cache**: Stores frequently asked queries and their results&#10;- **Embedding Cache**: Caches document embeddings to avoid recomputation&#10;- **Result Cache**: Stores generation results for specific query-document combinations&#10;- **Cache Invalidation Strategy**: Ensures cache coherence while maximizing performance&#10;&#10;#### Usage Example&#10;&#10;```python&#10;from src.models.cache_rag import CacheRAG&#10;&#10;# Initialize the model&#10;cache_rag = CacheRAG(&#10;    cache_size=10000,&#10;    ttl_minutes=10,&#10;    invalidation_policy=&quot;hybrid&quot;&#10;)&#10;&#10;# Process documents&#10;cache_rag.process_documents(documents)&#10;&#10;# Query the system&#10;results = cache_rag.query(&quot;Summarize recent port scan activities&quot;)&#10;print(results)&#10;```&#10;&#10;### Hybrid Cache-Graph RAG&#10;&#10;Our novel Hybrid Cache-Graph RAG architecture combines the contextual understanding of Graph RAG with the performance benefits of Cache RAG.&#10;&#10;#### Key Components&#10;&#10;- **Integrated Knowledge Representation**: Maintains both vector embeddings and a knowledge graph&#10;- **Adaptive Retrieval Strategy**: Dynamically selects the optimal retrieval strategy&#10;- **Graph-Aware Caching**: Enhances caching with graph awareness&#10;- **Adaptive Component Weighting**: Dynamically adjusts component weights based on historical performance&#10;&#10;#### Usage Example&#10;&#10;```python&#10;from src.models.hybrid_rag import HybridCacheGraphRAG&#10;&#10;# Initialize the model&#10;hybrid_rag = HybridCacheGraphRAG(&#10;    component_weights=&quot;adaptive&quot;,&#10;    integration_strategy=&quot;hierarchical&quot;,&#10;    adaptation_rate=&quot;medium&quot;&#10;)&#10;&#10;# Process documents and build graph&#10;hybrid_rag.process_data(network_data)&#10;&#10;# Query the system&#10;results = hybrid_rag.query(&quot;Analyze potential data exfiltration in this traffic pattern&quot;)&#10;print(results)&#10;```&#10;&#10;## Data Processing&#10;&#10;The data processing module handles the preparation of network security datasets for use with the RAG architectures.&#10;&#10;### Dataset Processors&#10;&#10;- **CIC-IDS2017 Processor**: Processes the CIC-IDS2017 dataset&#10;- **UNSW-NB15 Processor**: Processes the UNSW-NB15 dataset&#10;- **PCAP Processor**: Processes custom PCAP files&#10;&#10;### Usage Example&#10;&#10;```python&#10;from src.data_processing.dataset_processing import CICProcessor, UNSWProcessor, PCAPProcessor&#10;&#10;# Process CIC-IDS2017 dataset&#10;cic_processor = CICProcessor(raw_path=&quot;data/raw/cic-ids2017&quot;, processed_path=&quot;data/processed/cic-ids2017&quot;)&#10;cic_data = cic_processor.process()&#10;&#10;# Process UNSW-NB15 dataset&#10;unsw_processor = UNSWProcessor(raw_path=&quot;data/raw/unsw-nb15&quot;, processed_path=&quot;data/processed/unsw-nb15&quot;)&#10;unsw_data = unsw_processor.process()&#10;&#10;# Process custom PCAP files&#10;pcap_processor = PCAPProcessor(raw_path=&quot;data/raw/custom-pcaps&quot;, processed_path=&quot;data/processed/custom-pcaps&quot;)&#10;pcap_data = pcap_processor.process()&#10;```&#10;&#10;### Feature Extraction&#10;&#10;The feature extraction module extracts relevant features from network packets and flows.&#10;&#10;```python&#10;from src.data_processing.feature_extraction import PacketFeatureExtractor&#10;&#10;# Initialize feature extractor&#10;extractor = PacketFeatureExtractor()&#10;&#10;# Extract features from packets&#10;features = extractor.extract_features(packets)&#10;```&#10;&#10;### Data Splitting&#10;&#10;The data splitting module handles the division of datasets into training and testing sets.&#10;&#10;```python&#10;from src.data_processing.data_splitting import DataSplitter&#10;&#10;# Initialize data splitter&#10;splitter = DataSplitter(test_size=0.2, random_state=42)&#10;&#10;# Split data into training and testing sets&#10;train_data, test_data = splitter.split(processed_data)&#10;```&#10;&#10;## Evaluation&#10;&#10;The evaluation module provides tools for benchmarking and testing the RAG architectures.&#10;&#10;### Metrics&#10;&#10;- **Retrieval Metrics**: MAP, MRR, Precision@k, Recall@k&#10;- **Classification Metrics**: Precision, Recall, F1-score, AUC&#10;- **Efficiency Metrics**: Latency, Throughput, Memory usage, CPU utilization&#10;&#10;### Test Scenarios&#10;&#10;- **Standard Scenario**: Evaluates performance on known attack types&#10;- **Zero-Day Scenario**: Tests ability to detect novel threats&#10;- **High-Throughput Scenario**: Evaluates scalability under high traffic volumes&#10;- **Adversarial Scenario**: Tests resilience against evasion techniques&#10;&#10;### Usage Example&#10;&#10;```python&#10;from src.evaluation.benchmark import Benchmark&#10;from src.evaluation.metrics import RetrievalMetrics, ClassificationMetrics, EfficiencyMetrics&#10;&#10;# Initialize benchmark&#10;benchmark = Benchmark(&#10;    models=[trad_rag, graph_rag, cache_rag, hybrid_rag],&#10;    metrics=[RetrievalMetrics(), ClassificationMetrics(), EfficiencyMetrics()],&#10;    scenarios=[&quot;standard&quot;, &quot;zero_day&quot;, &quot;high_throughput&quot;, &quot;adversarial&quot;]&#10;)&#10;&#10;# Run benchmark&#10;results = benchmark.run(test_data)&#10;&#10;# Print results&#10;benchmark.print_results(results)&#10;```&#10;&#10;### Ablation Studies&#10;&#10;The ablation module allows for isolating the contribution of individual components in each architecture.&#10;&#10;```python&#10;from src.evaluation.ablation import AblationStudy&#10;&#10;# Initialize ablation study&#10;ablation = AblationStudy(model=hybrid_rag)&#10;&#10;# Define components to ablate&#10;components = {&#10;    &quot;component_weights&quot;: [&quot;fixed&quot;, &quot;adaptive&quot;],&#10;    &quot;integration_strategy&quot;: [&quot;parallel&quot;, &quot;sequential&quot;, &quot;hierarchical&quot;],&#10;    &quot;adaptation_rate&quot;: [&quot;fast&quot;, &quot;medium&quot;, &quot;slow&quot;]&#10;}&#10;&#10;# Run ablation study&#10;ablation_results = ablation.run(components, test_data)&#10;&#10;# Print results&#10;ablation.print_results(ablation_results)&#10;```&#10;&#10;## Visualization&#10;&#10;The visualization module provides tools for generating visualizations of the RAG architectures and their performance.&#10;&#10;### System Architecture Diagrams&#10;&#10;```python&#10;from src.visualization.system_architecture import ArchitectureDiagramGenerator&#10;&#10;# Initialize diagram generator&#10;diagram_gen = ArchitectureDiagramGenerator()&#10;&#10;# Generate diagrams&#10;diagram_gen.generate_traditional_rag_diagram(output_path=&quot;papers/figures/system_architecture/traditional_rag.svg&quot;)&#10;diagram_gen.generate_graph_rag_diagram(output_path=&quot;papers/figures/system_architecture/graph_rag.svg&quot;)&#10;diagram_gen.generate_cache_rag_diagram(output_path=&quot;papers/figures/system_architecture/cache_rag.svg&quot;)&#10;diagram_gen.generate_hybrid_rag_diagram(output_path=&quot;papers/figures/system_architecture/hybrid_cache_graph_rag.svg&quot;)&#10;```&#10;&#10;### Performance Charts&#10;&#10;```python&#10;from src.visualization.performance_charts import PerformanceChartGenerator&#10;&#10;# Initialize chart generator&#10;chart_gen = PerformanceChartGenerator()&#10;&#10;# Generate performance charts&#10;chart_gen.generate_accuracy_comparison(results, output_path=&quot;papers/figures/performance_comparison/accuracy_comparison.svg&quot;)&#10;chart_gen.generate_roc_curves(results, output_path=&quot;papers/figures/performance_comparison/roc_curves.svg&quot;)&#10;chart_gen.generate_pr_curves(results, output_path=&quot;papers/figures/performance_comparison/pr_curves.svg&quot;)&#10;chart_gen.generate_latency_comparison(results, output_path=&quot;papers/figures/performance_comparison/latency_comparison.svg&quot;)&#10;```&#10;&#10;### Knowledge Graph Visualizations&#10;&#10;```python&#10;from src.visualization.knowledge_graphs import KnowledgeGraphVisualizer&#10;&#10;# Initialize visualizer&#10;kg_vis = KnowledgeGraphVisualizer()&#10;&#10;# Generate knowledge graph visualizations&#10;kg_vis.visualize_entity_relationships(graph, output_path=&quot;papers/figures/knowledge_graphs/entity_relationships.svg&quot;)&#10;kg_vis.visualize_communication_patterns(graph, output_path=&quot;papers/figures/knowledge_graphs/communication_patterns.svg&quot;)&#10;kg_vis.visualize_subgraph_extraction(graph, query, output_path=&quot;papers/figures/knowledge_graphs/subgraph_extraction.svg&quot;)&#10;```&#10;&#10;### Cache Performance Visualizations&#10;&#10;```python&#10;from src.visualization.cache_performance import CachePerformanceVisualizer&#10;&#10;# Initialize visualizer&#10;cache_vis = CachePerformanceVisualizer()&#10;&#10;# Generate cache performance visualizations&#10;cache_vis.visualize_hit_rate_heatmap(cache_results, output_path=&quot;papers/figures/cache_performance/hit_rate_size_complexity.svg&quot;)&#10;cache_vis.visualize_invalidation_timeline(cache_results, output_path=&quot;papers/figures/cache_performance/invalidation_timeline.svg&quot;)&#10;cache_vis.visualize_time_series(cache_results, output_path=&quot;papers/figures/cache_performance/hit_rate_time_series.svg&quot;)&#10;```&#10;&#10;### Graph Traversal Analytics&#10;&#10;```python&#10;from src.visualization.graph_traversal import GraphTraversalVisualizer&#10;&#10;# Initialize visualizer&#10;graph_vis = GraphTraversalVisualizer()&#10;&#10;# Generate graph traversal visualizations&#10;graph_vis.visualize_hop_distance_histogram(graph_results, output_path=&quot;papers/figures/graph_traversal/hop_distance_histogram.svg&quot;)&#10;graph_vis.visualize_hop_count_accuracy(graph_results, output_path=&quot;papers/figures/graph_traversal/hop_count_accuracy.svg&quot;)&#10;graph_vis.visualize_topk_optimization(graph_results, output_path=&quot;papers/figures/graph_traversal/topk_optimization.svg&quot;)&#10;```&#10;&#10;### Experimental Results Visualizations&#10;&#10;```python&#10;from src.visualization.experimental_results import ExperimentalResultsVisualizer&#10;&#10;# Initialize visualizer&#10;exp_vis = ExperimentalResultsVisualizer()&#10;&#10;# Generate experimental results visualizations&#10;exp_vis.visualize_scenario_comparison(results, output_path=&quot;papers/figures/experimental_results/scenario_comparison.svg&quot;)&#10;exp_vis.visualize_ablation_study(ablation_results, output_path=&quot;papers/figures/experimental_results/ablation_study_hybrid_cache-graph_rag.svg&quot;)&#10;exp_vis.visualize_confusion_matrices(results, output_path=&quot;papers/figures/experimental_results/confusion_matrix_hybrid_cache-graph_rag.svg&quot;)&#10;exp_vis.visualize_significance_tests(significance_results, output_path=&quot;papers/figures/experimental_results/significance_pvalues.svg&quot;)&#10;```&#10;&#10;## Experiments&#10;&#10;The experiments module provides tools for running experiments with the RAG architectures.&#10;&#10;### Configuration&#10;&#10;Experiment configurations are stored in YAML files in the `experiments/configs` directory.&#10;&#10;Example configuration file (`experiments/configs/hybrid_rag_zero_day.yaml`):&#10;```yaml&#10;model:&#10;  type: hybrid_cache_graph_rag&#10;  params:&#10;    component_weights: adaptive&#10;    integration_strategy: hierarchical&#10;    adaptation_rate: medium&#10;&#10;dataset:&#10;  name: custom_pcaps&#10;  scenario: zero_day&#10;  split:&#10;    test_size: 0.2&#10;    random_state: 42&#10;&#10;evaluation:&#10;  metrics:&#10;    - map&#10;    - mrr&#10;    - precision@10&#10;    - recall@10&#10;    - f1_score&#10;    - auc&#10;    - latency&#10;    - throughput&#10;  n_runs: 5&#10;  significance_test: true&#10;```&#10;&#10;### Running Experiments&#10;&#10;```python&#10;from src.experiments.run_experiments import ExperimentRunner&#10;&#10;# Initialize experiment runner&#10;runner = ExperimentRunner()&#10;&#10;# Run experiment from configuration file&#10;results = runner.run_from_config(&quot;experiments/configs/hybrid_rag_zero_day.yaml&quot;)&#10;&#10;# Save results&#10;runner.save_results(results, &quot;experiments/results/hybrid_rag_zero_day_results.json&quot;)&#10;```&#10;&#10;### Analyzing Results&#10;&#10;```python&#10;from src.experiments.analyze_results import ResultAnalyzer&#10;&#10;# Initialize result analyzer&#10;analyzer = ResultAnalyzer()&#10;&#10;# Load results&#10;results = analyzer.load_results(&quot;experiments/results/hybrid_rag_zero_day_results.json&quot;)&#10;&#10;# Analyze results&#10;analysis = analyzer.analyze(results)&#10;&#10;# Generate report&#10;analyzer.generate_report(analysis, &quot;experiments/results/hybrid_rag_zero_day_report.md&quot;)&#10;```&#10;&#10;## API Reference&#10;&#10;### Models&#10;&#10;#### TraditionalRAG&#10;&#10;```python&#10;class TraditionalRAG:&#10;    def __init__(self, embedding_dim=768, chunk_strategy=&quot;sliding_window&quot;, retrieval_method=&quot;dense&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the Traditional RAG model.&#10;        &#10;        Args:&#10;            embedding_dim (int): Dimension of the embedding vectors&#10;            chunk_strategy (str): Strategy for chunking documents (&quot;fixed_size&quot;, &quot;sliding_window&quot;, &quot;semantic&quot;)&#10;            retrieval_method (str): Method for retrieval (&quot;bm25&quot;, &quot;dense&quot;, &quot;hybrid&quot;)&#10;        &quot;&quot;&quot;&#10;        &#10;    def process_documents(self, documents):&#10;        &quot;&quot;&quot;&#10;        Process documents for retrieval.&#10;        &#10;        Args:&#10;            documents (list): List of documents to process&#10;        &quot;&quot;&quot;&#10;        &#10;    def query(self, query_text):&#10;        &quot;&quot;&quot;&#10;        Query the system.&#10;        &#10;        Args:&#10;            query_text (str): Query text&#10;            &#10;        Returns:&#10;            dict: Query results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### GraphRAG&#10;&#10;```python&#10;class GraphRAG:&#10;    def __init__(self, graph_type=&quot;heterogeneous&quot;, node_config=&quot;entity_attribute&quot;, traversal_depth=2):&#10;        &quot;&quot;&quot;&#10;        Initialize the Graph RAG model.&#10;        &#10;        Args:&#10;            graph_type (str): Type of graph (&quot;homogeneous&quot;, &quot;heterogeneous&quot;)&#10;            node_config (str): Node configuration (&quot;entity_only&quot;, &quot;entity_attribute&quot;)&#10;            traversal_depth (int): Depth of graph traversal&#10;        &quot;&quot;&quot;&#10;        &#10;    def build_graph(self, network_data):&#10;        &quot;&quot;&quot;&#10;        Build the knowledge graph.&#10;        &#10;        Args:&#10;            network_data (dict): Network data to build the graph from&#10;        &quot;&quot;&quot;&#10;        &#10;    def query(self, query_text):&#10;        &quot;&quot;&quot;&#10;        Query the system.&#10;        &#10;        Args:&#10;            query_text (str): Query text&#10;            &#10;        Returns:&#10;            dict: Query results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### CacheRAG&#10;&#10;```python&#10;class CacheRAG:&#10;    def __init__(self, cache_size=10000, ttl_minutes=10, invalidation_policy=&quot;hybrid&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the Cache RAG model.&#10;        &#10;        Args:&#10;            cache_size (int): Size of the cache in number of entries&#10;            ttl_minutes (int): Time-to-live in minutes&#10;            invalidation_policy (str): Cache invalidation policy (&quot;time_based&quot;, &quot;change_based&quot;, &quot;hybrid&quot;)&#10;        &quot;&quot;&quot;&#10;        &#10;    def process_documents(self, documents):&#10;        &quot;&quot;&quot;&#10;        Process documents for retrieval.&#10;        &#10;        Args:&#10;            documents (list): List of documents to process&#10;        &quot;&quot;&quot;&#10;        &#10;    def query(self, query_text):&#10;        &quot;&quot;&quot;&#10;        Query the system.&#10;        &#10;        Args:&#10;            query_text (str): Query text&#10;            &#10;        Returns:&#10;            dict: Query results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### HybridCacheGraphRAG&#10;&#10;```python&#10;class HybridCacheGraphRAG:&#10;    def __init__(self, component_weights=&quot;adaptive&quot;, integration_strategy=&quot;hierarchical&quot;, adaptation_rate=&quot;medium&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the Hybrid Cache-Graph RAG model.&#10;        &#10;        Args:&#10;            component_weights (str): Component weighting strategy (&quot;fixed&quot;, &quot;adaptive&quot;)&#10;            integration_strategy (str): Integration strategy (&quot;parallel&quot;, &quot;sequential&quot;, &quot;hierarchical&quot;)&#10;            adaptation_rate (str): Rate of adaptation (&quot;fast&quot;, &quot;medium&quot;, &quot;slow&quot;)&#10;        &quot;&quot;&quot;&#10;        &#10;    def process_data(self, network_data):&#10;        &quot;&quot;&quot;&#10;        Process data for retrieval and build the graph.&#10;        &#10;        Args:&#10;            network_data (dict): Network data to process&#10;        &quot;&quot;&quot;&#10;        &#10;    def query(self, query_text):&#10;        &quot;&quot;&quot;&#10;        Query the system.&#10;        &#10;        Args:&#10;            query_text (str): Query text&#10;            &#10;        Returns:&#10;            dict: Query results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;### Data Processing&#10;&#10;#### DatasetProcessor&#10;&#10;```python&#10;class DatasetProcessor:&#10;    def __init__(self, raw_path, processed_path):&#10;        &quot;&quot;&quot;&#10;        Initialize the dataset processor.&#10;        &#10;        Args:&#10;            raw_path (str): Path to raw dataset&#10;            processed_path (str): Path to save processed dataset&#10;        &quot;&quot;&quot;&#10;        &#10;    def process(self):&#10;        &quot;&quot;&quot;&#10;        Process the dataset.&#10;        &#10;        Returns:&#10;            dict: Processed data&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### FeatureExtractor&#10;&#10;```python&#10;class FeatureExtractor:&#10;    def __init__(self):&#10;        &quot;&quot;&quot;&#10;        Initialize the feature extractor.&#10;        &quot;&quot;&quot;&#10;        &#10;    def extract_features(self, data):&#10;        &quot;&quot;&quot;&#10;        Extract features from data.&#10;        &#10;        Args:&#10;            data (dict): Data to extract features from&#10;            &#10;        Returns:&#10;            dict: Extracted features&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### DataSplitter&#10;&#10;```python&#10;class DataSplitter:&#10;    def __init__(self, test_size=0.2, random_state=42):&#10;        &quot;&quot;&quot;&#10;        Initialize the data splitter.&#10;        &#10;        Args:&#10;            test_size (float): Proportion of data to use for testing&#10;            random_state (int): Random seed for reproducibility&#10;        &quot;&quot;&quot;&#10;        &#10;    def split(self, data):&#10;        &quot;&quot;&quot;&#10;        Split data into training and testing sets.&#10;        &#10;        Args:&#10;            data (dict): Data to split&#10;            &#10;        Returns:&#10;            tuple: (train_data, test_data)&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;### Evaluation&#10;&#10;#### Benchmark&#10;&#10;```python&#10;class Benchmark:&#10;    def __init__(self, models, metrics, scenarios):&#10;        &quot;&quot;&quot;&#10;        Initialize the benchmark.&#10;        &#10;        Args:&#10;            models (list): List of models to benchmark&#10;            metrics (list): List of metrics to compute&#10;            scenarios (list): List of test scenarios&#10;        &quot;&quot;&quot;&#10;        &#10;    def run(self, test_data):&#10;        &quot;&quot;&quot;&#10;        Run the benchmark.&#10;        &#10;        Args:&#10;            test_data (dict): Test data&#10;            &#10;        Returns:&#10;            dict: Benchmark results&#10;        &quot;&quot;&quot;&#10;        &#10;    def print_results(self, results):&#10;        &quot;&quot;&quot;&#10;        Print benchmark results.&#10;        &#10;        Args:&#10;            results (dict): Benchmark results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### AblationStudy&#10;&#10;```python&#10;class AblationStudy:&#10;    def __init__(self, model):&#10;        &quot;&quot;&quot;&#10;        Initialize the ablation study.&#10;        &#10;        Args:&#10;            model: Model to study&#10;        &quot;&quot;&quot;&#10;        &#10;    def run(self, components, test_data):&#10;        &quot;&quot;&quot;&#10;        Run the ablation study.&#10;        &#10;        Args:&#10;            components (dict): Components to ablate&#10;            test_data (dict): Test data&#10;            &#10;        Returns:&#10;            dict: Ablation study results&#10;        &quot;&quot;&quot;&#10;        &#10;    def print_results(self, results):&#10;        &quot;&quot;&quot;&#10;        Print ablation study results.&#10;        &#10;        Args:&#10;            results (dict): Ablation study results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;### Visualization&#10;&#10;#### VisualizationBase&#10;&#10;```python&#10;class VisualizationBase:&#10;    def __init__(self):&#10;        &quot;&quot;&quot;&#10;        Initialize the visualization base.&#10;        &quot;&quot;&quot;&#10;        &#10;    def save_figure(self, fig, output_path):&#10;        &quot;&quot;&quot;&#10;        Save figure to file.&#10;        &#10;        Args:&#10;            fig: Figure to save&#10;            output_path (str): Path to save figure&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;## Contributing&#10;&#10;We welcome contributions to this project! Please follow these steps to contribute:&#10;&#10;1. Fork the repository&#10;2. Create a feature branch (`git checkout -b feature/your-feature`)&#10;3. Commit your changes (`git commit -m 'Add your feature'`)&#10;4. Push to the branch (`git push origin feature/your-feature`)&#10;5. Open a Pull Request&#10;&#10;Please ensure your code follows our coding standards and includes appropriate tests and documentation.&#10;&#10;---&#10;&#10;For more information, please contact the project maintainers at [contact@network-rag-research.org](mailto:contact@network-rag-research.org).&#10;" />
              <option name="updatedContent" value="# Network Security RAG Comparison - Code Documentation&#10;&#10;**Contact:** Dr Muhammad M.Hanafy  &#10;**Email:** m.mahmoud@mau.edu.eg&#10;&#10;This document provides comprehensive documentation for the code implementation of our network security RAG comparison research project. The documentation covers all components of the system, including models, data processing, evaluation, and visualization.&#10;&#10;## Table of Contents&#10;1. [Project Overview](#project-overview)&#10;2. [Installation](#installation)&#10;3. [Directory Structure](#directory-structure)&#10;4. [Models](#models)&#10;5. [Data Processing](#data-processing)&#10;6. [Evaluation](#evaluation)&#10;7. [Visualization](#visualization)&#10;8. [Experiments](#experiments)&#10;9. [API Reference](#api-reference)&#10;10. [Contributing](#contributing)&#10;&#10;## Project Overview&#10;&#10;This project implements and compares four Retrieval Augmented Generation (RAG) architectures for network security packet analysis:&#10;&#10;1. **Traditional RAG**: A vector-based retrieval system optimized for network security data&#10;2. **Graph RAG**: A graph-based approach that models network entities and their relationships&#10;3. **Cache RAG**: A performance-optimized system with strategic caching mechanisms&#10;4. **Hybrid Cache-Graph RAG**: A novel approach combining graph-based knowledge representation with caching benefits&#10;&#10;The project includes implementations of all four architectures, data processing pipelines, evaluation frameworks, and visualization tools.&#10;&#10;## Installation&#10;&#10;### Prerequisites&#10;- Python 3.8+&#10;- CUDA-compatible GPU (recommended for optimal performance)&#10;- 16GB+ RAM&#10;- 100GB+ disk space&#10;&#10;### Setup with Docker (Recommended)&#10;```bash&#10;# Clone the repository&#10;git clone https://github.com/network-rag-research/network-security-rag-comparison.git&#10;cd network-rag-research&#10;&#10;# Build and run the Docker container&#10;docker build -t network-rag-research .&#10;docker run -it --gpus all -v $(pwd):/network-rag-research network-rag-research&#10;```&#10;&#10;### Manual Setup&#10;```bash&#10;# Clone the repository&#10;git clone https://github.com/network-rag-research/network-security-rag-comparison.git&#10;cd network-rag-research&#10;&#10;# Create and activate a virtual environment&#10;python -m venv venv&#10;source venv/bin/activate  # On Windows: venv\Scripts\activate&#10;&#10;# Install dependencies&#10;pip install -r requirements.txt&#10;```&#10;&#10;## Directory Structure&#10;&#10;```&#10;/network-rag-research/&#10;├── src/&#10;│   ├── models/              # Implementation of all RAG variants&#10;│   ├── data_processing/     # Data preprocessing and loading utilities&#10;│   ├── evaluation/          # Benchmarking and testing code&#10;│   ├── visualization/       # Plotting and graph generation code&#10;│   └── utils/               # Helper functions and common utilities&#10;├── data/&#10;│   ├── raw/                 # Original datasets&#10;│   ├── processed/           # Cleaned and preprocessed data&#10;│   └── embeddings/          # Vector representations and indexes&#10;├── experiments/&#10;│   ├── configs/             # Configuration files for different experiments&#10;│   ├── logs/                # Training and evaluation logs&#10;│   └── results/             # Metrics and performance data&#10;└── docker/                  # Containerization files for reproducibility&#10;```&#10;&#10;## Models&#10;&#10;### Traditional RAG&#10;&#10;The Traditional RAG architecture follows the standard retrieval-augmented generation paradigm, adapted for network security packet analysis.&#10;&#10;#### Key Components&#10;&#10;- **Document Processing**: Converts network packets and flow records into document chunks&#10;- **Embedding Generation**: Encodes documents into dense vector representations&#10;- **Vector Index**: Stores embeddings for efficient similarity search&#10;- **Generation**: Uses retrieved documents as context for a language model&#10;&#10;#### Usage Example&#10;&#10;```python&#10;from src.models.traditional_rag import TraditionalRAG&#10;&#10;# Initialize the model&#10;trad_rag = TraditionalRAG(&#10;    embedding_dim=768,&#10;    chunk_strategy=&quot;sliding_window&quot;,&#10;    retrieval_method=&quot;dense&quot;&#10;)&#10;&#10;# Process documents&#10;trad_rag.process_documents(documents)&#10;&#10;# Query the system&#10;results = trad_rag.query(&quot;Detect potential DDoS attacks in this traffic&quot;)&#10;print(results)&#10;```&#10;&#10;### Graph RAG&#10;&#10;The Graph RAG architecture extends Traditional RAG by modeling network entities and their relationships as a knowledge graph.&#10;&#10;#### Key Components&#10;&#10;- **Knowledge Graph Construction**: Represents network entities as nodes in a heterogeneous graph&#10;- **Graph Embedding**: Generates node and edge embeddings using a graph neural network&#10;- **Graph Traversal Retrieval**: Performs multi-hop traversal to retrieve related entities&#10;- **Context Aggregation**: Aggregates information from retrieved nodes and their neighborhoods&#10;&#10;#### Usage Example&#10;&#10;```python&#10;from src.models.graph_rag import GraphRAG&#10;&#10;# Initialize the model&#10;graph_rag = GraphRAG(&#10;    graph_type=&quot;heterogeneous&quot;,&#10;    node_config=&quot;entity_attribute&quot;,&#10;    traversal_depth=2&#10;)&#10;&#10;# Build the knowledge graph&#10;graph_rag.build_graph(network_data)&#10;&#10;# Query the system&#10;results = graph_rag.query(&quot;Identify communication patterns between these IP addresses&quot;)&#10;print(results)&#10;```&#10;&#10;### Cache RAG&#10;&#10;The Cache RAG architecture focuses on performance optimization through strategic caching mechanisms.&#10;&#10;#### Key Components&#10;&#10;- **Query Cache**: Stores frequently asked queries and their results&#10;- **Embedding Cache**: Caches document embeddings to avoid recomputation&#10;- **Result Cache**: Stores generation results for specific query-document combinations&#10;- **Cache Invalidation Strategy**: Ensures cache coherence while maximizing performance&#10;&#10;#### Usage Example&#10;&#10;```python&#10;from src.models.cache_rag import CacheRAG&#10;&#10;# Initialize the model&#10;cache_rag = CacheRAG(&#10;    cache_size=10000,&#10;    ttl_minutes=10,&#10;    invalidation_policy=&quot;hybrid&quot;&#10;)&#10;&#10;# Process documents&#10;cache_rag.process_documents(documents)&#10;&#10;# Query the system&#10;results = cache_rag.query(&quot;Summarize recent port scan activities&quot;)&#10;print(results)&#10;```&#10;&#10;### Hybrid Cache-Graph RAG&#10;&#10;Our novel Hybrid Cache-Graph RAG architecture combines the contextual understanding of Graph RAG with the performance benefits of Cache RAG.&#10;&#10;#### Key Components&#10;&#10;- **Integrated Knowledge Representation**: Maintains both vector embeddings and a knowledge graph&#10;- **Adaptive Retrieval Strategy**: Dynamically selects the optimal retrieval strategy&#10;- **Graph-Aware Caching**: Enhances caching with graph awareness&#10;- **Adaptive Component Weighting**: Dynamically adjusts component weights based on historical performance&#10;&#10;#### Usage Example&#10;&#10;```python&#10;from src.models.hybrid_rag import HybridCacheGraphRAG&#10;&#10;# Initialize the model&#10;hybrid_rag = HybridCacheGraphRAG(&#10;    component_weights=&quot;adaptive&quot;,&#10;    integration_strategy=&quot;hierarchical&quot;,&#10;    adaptation_rate=&quot;medium&quot;&#10;)&#10;&#10;# Process documents and build graph&#10;hybrid_rag.process_data(network_data)&#10;&#10;# Query the system&#10;results = hybrid_rag.query(&quot;Analyze potential data exfiltration in this traffic pattern&quot;)&#10;print(results)&#10;```&#10;&#10;## Data Processing&#10;&#10;The data processing module handles the preparation of network security datasets for use with the RAG architectures.&#10;&#10;### Dataset Processors&#10;&#10;- **CIC-IDS2017 Processor**: Processes the CIC-IDS2017 dataset&#10;- **UNSW-NB15 Processor**: Processes the UNSW-NB15 dataset&#10;- **PCAP Processor**: Processes custom PCAP files&#10;&#10;### Usage Example&#10;&#10;```python&#10;from src.data_processing.dataset_processing import CICProcessor, UNSWProcessor, PCAPProcessor&#10;&#10;# Process CIC-IDS2017 dataset&#10;cic_processor = CICProcessor(raw_path=&quot;data/raw/cic-ids2017&quot;, processed_path=&quot;data/processed/cic-ids2017&quot;)&#10;cic_data = cic_processor.process()&#10;&#10;# Process UNSW-NB15 dataset&#10;unsw_processor = UNSWProcessor(raw_path=&quot;data/raw/unsw-nb15&quot;, processed_path=&quot;data/processed/unsw-nb15&quot;)&#10;unsw_data = unsw_processor.process()&#10;&#10;# Process custom PCAP files&#10;pcap_processor = PCAPProcessor(raw_path=&quot;data/raw/custom-pcaps&quot;, processed_path=&quot;data/processed/custom-pcaps&quot;)&#10;pcap_data = pcap_processor.process()&#10;```&#10;&#10;### Feature Extraction&#10;&#10;The feature extraction module extracts relevant features from network packets and flows.&#10;&#10;```python&#10;from src.data_processing.feature_extraction import PacketFeatureExtractor&#10;&#10;# Initialize feature extractor&#10;extractor = PacketFeatureExtractor()&#10;&#10;# Extract features from packets&#10;features = extractor.extract_features(packets)&#10;```&#10;&#10;### Data Splitting&#10;&#10;The data splitting module handles the division of datasets into training and testing sets.&#10;&#10;```python&#10;from src.data_processing.data_splitting import DataSplitter&#10;&#10;# Initialize data splitter&#10;splitter = DataSplitter(test_size=0.2, random_state=42)&#10;&#10;# Split data into training and testing sets&#10;train_data, test_data = splitter.split(processed_data)&#10;```&#10;&#10;## Evaluation&#10;&#10;The evaluation module provides tools for benchmarking and testing the RAG architectures.&#10;&#10;### Metrics&#10;&#10;- **Retrieval Metrics**: MAP, MRR, Precision@k, Recall@k&#10;- **Classification Metrics**: Precision, Recall, F1-score, AUC&#10;- **Efficiency Metrics**: Latency, Throughput, Memory usage, CPU utilization&#10;&#10;### Test Scenarios&#10;&#10;- **Standard Scenario**: Evaluates performance on known attack types&#10;- **Zero-Day Scenario**: Tests ability to detect novel threats&#10;- **High-Throughput Scenario**: Evaluates scalability under high traffic volumes&#10;- **Adversarial Scenario**: Tests resilience against evasion techniques&#10;&#10;### Usage Example&#10;&#10;```python&#10;from src.evaluation.benchmark import Benchmark&#10;from src.evaluation.metrics import RetrievalMetrics, ClassificationMetrics, EfficiencyMetrics&#10;&#10;# Initialize benchmark&#10;benchmark = Benchmark(&#10;    models=[trad_rag, graph_rag, cache_rag, hybrid_rag],&#10;    metrics=[RetrievalMetrics(), ClassificationMetrics(), EfficiencyMetrics()],&#10;    scenarios=[&quot;standard&quot;, &quot;zero_day&quot;, &quot;high_throughput&quot;, &quot;adversarial&quot;]&#10;)&#10;&#10;# Run benchmark&#10;results = benchmark.run(test_data)&#10;&#10;# Print results&#10;benchmark.print_results(results)&#10;```&#10;&#10;### Ablation Studies&#10;&#10;The ablation module allows for isolating the contribution of individual components in each architecture.&#10;&#10;```python&#10;from src.evaluation.ablation import AblationStudy&#10;&#10;# Initialize ablation study&#10;ablation = AblationStudy(model=hybrid_rag)&#10;&#10;# Define components to ablate&#10;components = {&#10;    &quot;component_weights&quot;: [&quot;fixed&quot;, &quot;adaptive&quot;],&#10;    &quot;integration_strategy&quot;: [&quot;parallel&quot;, &quot;sequential&quot;, &quot;hierarchical&quot;],&#10;    &quot;adaptation_rate&quot;: [&quot;fast&quot;, &quot;medium&quot;, &quot;slow&quot;]&#10;}&#10;&#10;# Run ablation study&#10;ablation_results = ablation.run(components, test_data)&#10;&#10;# Print results&#10;ablation.print_results(ablation_results)&#10;```&#10;&#10;## Visualization&#10;&#10;The visualization module provides tools for generating visualizations of the RAG architectures and their performance.&#10;&#10;### System Architecture Diagrams&#10;&#10;```python&#10;from src.visualization.system_architecture import ArchitectureDiagramGenerator&#10;&#10;# Initialize diagram generator&#10;diagram_gen = ArchitectureDiagramGenerator()&#10;&#10;# Generate diagrams&#10;diagram_gen.generate_traditional_rag_diagram(output_path=&quot;papers/figures/system_architecture/traditional_rag.svg&quot;)&#10;diagram_gen.generate_graph_rag_diagram(output_path=&quot;papers/figures/system_architecture/graph_rag.svg&quot;)&#10;diagram_gen.generate_cache_rag_diagram(output_path=&quot;papers/figures/system_architecture/cache_rag.svg&quot;)&#10;diagram_gen.generate_hybrid_rag_diagram(output_path=&quot;papers/figures/system_architecture/hybrid_cache_graph_rag.svg&quot;)&#10;```&#10;&#10;### Performance Charts&#10;&#10;```python&#10;from src.visualization.performance_charts import PerformanceChartGenerator&#10;&#10;# Initialize chart generator&#10;chart_gen = PerformanceChartGenerator()&#10;&#10;# Generate performance charts&#10;chart_gen.generate_accuracy_comparison(results, output_path=&quot;papers/figures/performance_comparison/accuracy_comparison.svg&quot;)&#10;chart_gen.generate_roc_curves(results, output_path=&quot;papers/figures/performance_comparison/roc_curves.svg&quot;)&#10;chart_gen.generate_pr_curves(results, output_path=&quot;papers/figures/performance_comparison/pr_curves.svg&quot;)&#10;chart_gen.generate_latency_comparison(results, output_path=&quot;papers/figures/performance_comparison/latency_comparison.svg&quot;)&#10;```&#10;&#10;### Knowledge Graph Visualizations&#10;&#10;```python&#10;from src.visualization.knowledge_graphs import KnowledgeGraphVisualizer&#10;&#10;# Initialize visualizer&#10;kg_vis = KnowledgeGraphVisualizer()&#10;&#10;# Generate knowledge graph visualizations&#10;kg_vis.visualize_entity_relationships(graph, output_path=&quot;papers/figures/knowledge_graphs/entity_relationships.svg&quot;)&#10;kg_vis.visualize_communication_patterns(graph, output_path=&quot;papers/figures/knowledge_graphs/communication_patterns.svg&quot;)&#10;kg_vis.visualize_subgraph_extraction(graph, query, output_path=&quot;papers/figures/knowledge_graphs/subgraph_extraction.svg&quot;)&#10;```&#10;&#10;### Cache Performance Visualizations&#10;&#10;```python&#10;from src.visualization.cache_performance import CachePerformanceVisualizer&#10;&#10;# Initialize visualizer&#10;cache_vis = CachePerformanceVisualizer()&#10;&#10;# Generate cache performance visualizations&#10;cache_vis.visualize_hit_rate_heatmap(cache_results, output_path=&quot;papers/figures/cache_performance/hit_rate_size_complexity.svg&quot;)&#10;cache_vis.visualize_invalidation_timeline(cache_results, output_path=&quot;papers/figures/cache_performance/invalidation_timeline.svg&quot;)&#10;cache_vis.visualize_time_series(cache_results, output_path=&quot;papers/figures/cache_performance/hit_rate_time_series.svg&quot;)&#10;```&#10;&#10;### Graph Traversal Analytics&#10;&#10;```python&#10;from src.visualization.graph_traversal import GraphTraversalVisualizer&#10;&#10;# Initialize visualizer&#10;graph_vis = GraphTraversalVisualizer()&#10;&#10;# Generate graph traversal visualizations&#10;graph_vis.visualize_hop_distance_histogram(graph_results, output_path=&quot;papers/figures/graph_traversal/hop_distance_histogram.svg&quot;)&#10;graph_vis.visualize_hop_count_accuracy(graph_results, output_path=&quot;papers/figures/graph_traversal/hop_count_accuracy.svg&quot;)&#10;graph_vis.visualize_topk_optimization(graph_results, output_path=&quot;papers/figures/graph_traversal/topk_optimization.svg&quot;)&#10;```&#10;&#10;### Experimental Results Visualizations&#10;&#10;```python&#10;from src.visualization.experimental_results import ExperimentalResultsVisualizer&#10;&#10;# Initialize visualizer&#10;exp_vis = ExperimentalResultsVisualizer()&#10;&#10;# Generate experimental results visualizations&#10;exp_vis.visualize_scenario_comparison(results, output_path=&quot;papers/figures/experimental_results/scenario_comparison.svg&quot;)&#10;exp_vis.visualize_ablation_study(ablation_results, output_path=&quot;papers/figures/experimental_results/ablation_study_hybrid_cache-graph_rag.svg&quot;)&#10;exp_vis.visualize_confusion_matrices(results, output_path=&quot;papers/figures/experimental_results/confusion_matrix_hybrid_cache-graph_rag.svg&quot;)&#10;exp_vis.visualize_significance_tests(significance_results, output_path=&quot;papers/figures/experimental_results/significance_pvalues.svg&quot;)&#10;```&#10;&#10;## Experiments&#10;&#10;The experiments module provides tools for running experiments with the RAG architectures.&#10;&#10;### Configuration&#10;&#10;Experiment configurations are stored in YAML files in the `experiments/configs` directory.&#10;&#10;Example configuration file (`experiments/configs/hybrid_rag_zero_day.yaml`):&#10;```yaml&#10;model:&#10;  type: hybrid_cache_graph_rag&#10;  params:&#10;    component_weights: adaptive&#10;    integration_strategy: hierarchical&#10;    adaptation_rate: medium&#10;&#10;dataset:&#10;  name: custom_pcaps&#10;  scenario: zero_day&#10;  split:&#10;    test_size: 0.2&#10;    random_state: 42&#10;&#10;evaluation:&#10;  metrics:&#10;    - map&#10;    - mrr&#10;    - precision@10&#10;    - recall@10&#10;    - f1_score&#10;    - auc&#10;    - latency&#10;    - throughput&#10;  n_runs: 5&#10;  significance_test: true&#10;```&#10;&#10;### Running Experiments&#10;&#10;```python&#10;from src.experiments.run_experiments import ExperimentRunner&#10;&#10;# Initialize experiment runner&#10;runner = ExperimentRunner()&#10;&#10;# Run experiment from configuration file&#10;results = runner.run_from_config(&quot;experiments/configs/hybrid_rag_zero_day.yaml&quot;)&#10;&#10;# Save results&#10;runner.save_results(results, &quot;experiments/results/hybrid_rag_zero_day_results.json&quot;)&#10;```&#10;&#10;### Analyzing Results&#10;&#10;```python&#10;from src.experiments.analyze_results import ResultAnalyzer&#10;&#10;# Initialize result analyzer&#10;analyzer = ResultAnalyzer()&#10;&#10;# Load results&#10;results = analyzer.load_results(&quot;experiments/results/hybrid_rag_zero_day_results.json&quot;)&#10;&#10;# Analyze results&#10;analysis = analyzer.analyze(results)&#10;&#10;# Generate report&#10;analyzer.generate_report(analysis, &quot;experiments/results/hybrid_rag_zero_day_report.md&quot;)&#10;```&#10;&#10;## API Reference&#10;&#10;### Models&#10;&#10;#### TraditionalRAG&#10;&#10;```python&#10;class TraditionalRAG:&#10;    def __init__(self, embedding_dim=768, chunk_strategy=&quot;sliding_window&quot;, retrieval_method=&quot;dense&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the Traditional RAG model.&#10;        &#10;        Args:&#10;            embedding_dim (int): Dimension of the embedding vectors&#10;            chunk_strategy (str): Strategy for chunking documents (&quot;fixed_size&quot;, &quot;sliding_window&quot;, &quot;semantic&quot;)&#10;            retrieval_method (str): Method for retrieval (&quot;bm25&quot;, &quot;dense&quot;, &quot;hybrid&quot;)&#10;        &quot;&quot;&quot;&#10;        &#10;    def process_documents(self, documents):&#10;        &quot;&quot;&quot;&#10;        Process documents for retrieval.&#10;        &#10;        Args:&#10;            documents (list): List of documents to process&#10;        &quot;&quot;&quot;&#10;        &#10;    def query(self, query_text):&#10;        &quot;&quot;&quot;&#10;        Query the system.&#10;        &#10;        Args:&#10;            query_text (str): Query text&#10;            &#10;        Returns:&#10;            dict: Query results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### GraphRAG&#10;&#10;```python&#10;class GraphRAG:&#10;    def __init__(self, graph_type=&quot;heterogeneous&quot;, node_config=&quot;entity_attribute&quot;, traversal_depth=2):&#10;        &quot;&quot;&quot;&#10;        Initialize the Graph RAG model.&#10;        &#10;        Args:&#10;            graph_type (str): Type of graph (&quot;homogeneous&quot;, &quot;heterogeneous&quot;)&#10;            node_config (str): Node configuration (&quot;entity_only&quot;, &quot;entity_attribute&quot;)&#10;            traversal_depth (int): Depth of graph traversal&#10;        &quot;&quot;&quot;&#10;        &#10;    def build_graph(self, network_data):&#10;        &quot;&quot;&quot;&#10;        Build the knowledge graph.&#10;        &#10;        Args:&#10;            network_data (dict): Network data to build the graph from&#10;        &quot;&quot;&quot;&#10;        &#10;    def query(self, query_text):&#10;        &quot;&quot;&quot;&#10;        Query the system.&#10;        &#10;        Args:&#10;            query_text (str): Query text&#10;            &#10;        Returns:&#10;            dict: Query results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### CacheRAG&#10;&#10;```python&#10;class CacheRAG:&#10;    def __init__(self, cache_size=10000, ttl_minutes=10, invalidation_policy=&quot;hybrid&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the Cache RAG model.&#10;        &#10;        Args:&#10;            cache_size (int): Size of the cache in number of entries&#10;            ttl_minutes (int): Time-to-live in minutes&#10;            invalidation_policy (str): Cache invalidation policy (&quot;time_based&quot;, &quot;change_based&quot;, &quot;hybrid&quot;)&#10;        &quot;&quot;&quot;&#10;        &#10;    def process_documents(self, documents):&#10;        &quot;&quot;&quot;&#10;        Process documents for retrieval.&#10;        &#10;        Args:&#10;            documents (list): List of documents to process&#10;        &quot;&quot;&quot;&#10;        &#10;    def query(self, query_text):&#10;        &quot;&quot;&quot;&#10;        Query the system.&#10;        &#10;        Args:&#10;            query_text (str): Query text&#10;            &#10;        Returns:&#10;            dict: Query results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### HybridCacheGraphRAG&#10;&#10;```python&#10;class HybridCacheGraphRAG:&#10;    def __init__(self, component_weights=&quot;adaptive&quot;, integration_strategy=&quot;hierarchical&quot;, adaptation_rate=&quot;medium&quot;):&#10;        &quot;&quot;&quot;&#10;        Initialize the Hybrid Cache-Graph RAG model.&#10;        &#10;        Args:&#10;            component_weights (str): Component weighting strategy (&quot;fixed&quot;, &quot;adaptive&quot;)&#10;            integration_strategy (str): Integration strategy (&quot;parallel&quot;, &quot;sequential&quot;, &quot;hierarchical&quot;)&#10;            adaptation_rate (str): Rate of adaptation (&quot;fast&quot;, &quot;medium&quot;, &quot;slow&quot;)&#10;        &quot;&quot;&quot;&#10;        &#10;    def process_data(self, network_data):&#10;        &quot;&quot;&quot;&#10;        Process data for retrieval and build the graph.&#10;        &#10;        Args:&#10;            network_data (dict): Network data to process&#10;        &quot;&quot;&quot;&#10;        &#10;    def query(self, query_text):&#10;        &quot;&quot;&quot;&#10;        Query the system.&#10;        &#10;        Args:&#10;            query_text (str): Query text&#10;            &#10;        Returns:&#10;            dict: Query results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;### Data Processing&#10;&#10;#### DatasetProcessor&#10;&#10;```python&#10;class DatasetProcessor:&#10;    def __init__(self, raw_path, processed_path):&#10;        &quot;&quot;&quot;&#10;        Initialize the dataset processor.&#10;        &#10;        Args:&#10;            raw_path (str): Path to raw dataset&#10;            processed_path (str): Path to save processed dataset&#10;        &quot;&quot;&quot;&#10;        &#10;    def process(self):&#10;        &quot;&quot;&quot;&#10;        Process the dataset.&#10;        &#10;        Returns:&#10;            dict: Processed data&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### FeatureExtractor&#10;&#10;```python&#10;class FeatureExtractor:&#10;    def __init__(self):&#10;        &quot;&quot;&quot;&#10;        Initialize the feature extractor.&#10;        &quot;&quot;&quot;&#10;        &#10;    def extract_features(self, data):&#10;        &quot;&quot;&quot;&#10;        Extract features from data.&#10;        &#10;        Args:&#10;            data (dict): Data to extract features from&#10;            &#10;        Returns:&#10;            dict: Extracted features&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### DataSplitter&#10;&#10;```python&#10;class DataSplitter:&#10;    def __init__(self, test_size=0.2, random_state=42):&#10;        &quot;&quot;&quot;&#10;        Initialize the data splitter.&#10;        &#10;        Args:&#10;            test_size (float): Proportion of data to use for testing&#10;            random_state (int): Random seed for reproducibility&#10;        &quot;&quot;&quot;&#10;        &#10;    def split(self, data):&#10;        &quot;&quot;&quot;&#10;        Split data into training and testing sets.&#10;        &#10;        Args:&#10;            data (dict): Data to split&#10;            &#10;        Returns:&#10;            tuple: (train_data, test_data)&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;### Evaluation&#10;&#10;#### Benchmark&#10;&#10;```python&#10;class Benchmark:&#10;    def __init__(self, models, metrics, scenarios):&#10;        &quot;&quot;&quot;&#10;        Initialize the benchmark.&#10;        &#10;        Args:&#10;            models (list): List of models to benchmark&#10;            metrics (list): List of metrics to compute&#10;            scenarios (list): List of test scenarios&#10;        &quot;&quot;&quot;&#10;        &#10;    def run(self, test_data):&#10;        &quot;&quot;&quot;&#10;        Run the benchmark.&#10;        &#10;        Args:&#10;            test_data (dict): Test data&#10;            &#10;        Returns:&#10;            dict: Benchmark results&#10;        &quot;&quot;&quot;&#10;        &#10;    def print_results(self, results):&#10;        &quot;&quot;&quot;&#10;        Print benchmark results.&#10;        &#10;        Args:&#10;            results (dict): Benchmark results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;#### AblationStudy&#10;&#10;```python&#10;class AblationStudy:&#10;    def __init__(self, model):&#10;        &quot;&quot;&quot;&#10;        Initialize the ablation study.&#10;        &#10;        Args:&#10;            model: Model to study&#10;        &quot;&quot;&quot;&#10;        &#10;    def run(self, components, test_data):&#10;        &quot;&quot;&quot;&#10;        Run the ablation study.&#10;        &#10;        Args:&#10;            components (dict): Components to ablate&#10;            test_data (dict): Test data&#10;            &#10;        Returns:&#10;            dict: Ablation study results&#10;        &quot;&quot;&quot;&#10;        &#10;    def print_results(self, results):&#10;        &quot;&quot;&quot;&#10;        Print ablation study results.&#10;        &#10;        Args:&#10;            results (dict): Ablation study results&#10;        &quot;&quot;&quot;&#10;```&#10;&#10;### Visualization&#10;&#10;#### VisualizationBase&#10;&#10;```python&#10;class VisualizationBase:&#10;    def __init__(self):&#10;        &quot;&quot;&quot;&#10;        Initialize the visualization base.&#10;        &quot;&quot;&quot;&#10;        &#10;    def save_figure(self, fig, output_path):&#10;        &quot;&quot;&quot;&#10;        Save figure to file.&#10;        &#10;        Args:&#10;            fig: Figure to save&#10;            output_path (str): Path to save figure&#10;        &quot;&quot;&quot;&#10;```" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>